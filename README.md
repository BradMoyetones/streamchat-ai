# üöÄ Multi-LLM Streaming Hub

Este proyecto es una interfaz moderna desarrollada en **Next.js** dise√±ada para interactuar con m√∫ltiples modelos de lenguaje de gran escala (LLMs) de forma fluida. Lo que hace especial a esta aplicaci√≥n es su capacidad de alternar din√°micamente entre diferentes proveedores de IA para garantizar disponibilidad y velocidad extrema.

---

### üß† El Concepto

La aplicaci√≥n se conecta a un backend optimizado que act√∫a como un **orquestador de servicios de IA**. En lugar de depender de una sola API, el sistema implementa una l√≥gica de "Round Robin" para distribuir las peticiones entre diversos proveedores, permitiendo:

* **Streaming en tiempo real:** Respuestas instant√°neas palabra por palabra.
* **Alta disponibilidad:** Si un servicio falla o alcanza su l√≠mite, el sistema utiliza el siguiente en la lista.
* **Multi-Model Support:** Integraci√≥n nativa con servicios de alto rendimiento como **Groq**, **Cerebras** y preparado para expandirse a Gemini, OpenRouter o modelos locales.

---

### üõ†Ô∏è Tecnolog√≠as Principales

* **Frontend:** [Next.js](https://nextjs.org/) (App Router) para una experiencia de usuario reactiva y optimizada.
* **Backend Runtime:** [Bun](https://bun.sh/) para un procesamiento de peticiones ultra r√°pido.
* **Protocolo:** **Server-Sent Events (SSE)** para el manejo de streams de datos constantes.
* **Proveedores de IA:** Integraci√≥n con APIs de baja latencia (Groq, Cerebras).

---

### üìã Caracter√≠sticas Clave

1. **Balanceo de Carga (Load Balancing):** Distribuci√≥n inteligente de mensajes entre diferentes servicios para maximizar el throughput.
2. **Interfaz de Chat Fluida:** Una UI limpia inspirada en los mejores est√°ndares actuales, optimizada para la lectura de respuestas largas en streaming.
3. **Arquitectura Extensible:** F√°cil de a√±adir nuevos servicios o modelos locales simplemente registr√°ndolos en el orquestador.
4. **Eficiencia Energ√©tica y de Memoria:** Gracias al uso de Bun en el lado del servidor, el consumo de recursos es m√≠nimo.

---

### üöÄ C√≥mo empezar

1. **Clona el repositorio**
2. **Instala las dependencias:** `npm install` o `bun install`.
3. **Configura tus variables de entorno:** A√±ade tus API Keys de los servicios correspondientes.
4. **Inicia el modo desarrollo:** `npm run dev` o `bun dev`.

---

> **Nota:** Este proyecto est√° dise√±ado para ser la base de una plataforma de chat agn√≥stica al modelo, enfoc√°ndose en la velocidad de respuesta y la resiliencia del servicio.